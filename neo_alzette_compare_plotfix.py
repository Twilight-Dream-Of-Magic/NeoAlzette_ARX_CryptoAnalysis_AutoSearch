from __future__ import annotations
from collections import defaultdict
from functools import lru_cache

from typing import Dict, Tuple
import random
import csv
import math

"""
Script to compare the differential complexities of the Alzette ARX-box and
the NeoAlzette ARX-box.  It implements simplified models of the two ARX
primitives at the difference level and computes the weight (negative
log‑probabilities) of a single round/box for a set of input differences.

The weight is expressed in bits and is the sum of the contributions of
all non‑linear operations (modular additions, subtractions and the
injection layer in NeoAlzette).  Linear operations such as rotations,
XOR with constants and diffusion layers are assumed to have zero
probability cost: they only permute the state or add constant
differences.

Due to the prohibitive cost of enumerating all 32‑bit differences,
the script samples a user defined number of random difference pairs
(delta_a, delta_b).  For each pair it computes the weight of the
Alzette round and the weight of the NeoAlzette round.  It then
emits a CSV file containing per input difference the two weights and
their difference and prints some aggregate statistics.  A simple plot
of the distribution of weight differences is also produced.

This script is self‑contained: all required arithmetic and mask
functions are implemented directly, and no external libraries are
needed beyond the Python standard library and matplotlib for the
plot.

The core pieces are:
    - Variable addition modelling following the LM‑2001 algorithm to
      compute the best output difference and its associated weight.
    - Modular subtraction by constant modelling using a greedy
      heuristic similar to the one used in the original test
      harness.  It selects an output difference for the subtraction
      that minimises the weight.
    - Dynamic diffusion masks and injection functions exactly matching
      the C++ constexpr reference (neoalzette_injection_constexpr.hpp).
      These allow modelling the cost and effect of the NOT‑* gate based
      cross-branch injection in NeoAlzette.
    - Simulation of a single Alzette round and a single NeoAlzette
      round in the difference domain, including updates of the delta
      values.

Note: because the injection mapping is affine, its weight is equal
to the rank of the linear subspace generated by the active bits in
the input branch.  This script computes the rank by inserting the
image of each active bit into a 32‑bit linear basis over GF(2).
"""


######################################################################
# Rotation helpers
######################################################################

def rotl32(x: int, r: int) -> int:
    """Rotate left 32‑bit integer x by r bits."""
    r &= 31
    return ((x << r) | (x >> (32 - r))) & 0xFFFFFFFF


def rotr32(x: int, r: int) -> int:
    """Rotate right 32‑bit integer x by r bits."""
    r &= 31
    return ((x >> r) | (x << (32 - r))) & 0xFFFFFFFF


######################################################################
# NOTE: legacy linear layers (L1/L2)
#
# Older drafts of the NeoAlzette spec inserted additional linear layers
# around injection. The current C++ reference implementation has removed
# those steps. We keep no L1/L2 propagation in the delta-round model below.
######################################################################

######################################################################
# Constants (ROUND_CONSTANTS) – reused from neoalzette_core.hpp
######################################################################

# NeoAlzette uses 16 round constants extracted from Fibonacci and other
# mathematical constants.  Only the first 12 are used in a single
# two‑subround box.  We place them here as a global so that the
# injection functions can reference them.
RC = [
    0x16B2C40B, 0xC117176A, 0x0F9A2598, 0xA1563ACA,
    0x243F6A88, 0x85A308D3, 0x13198102, 0xE0370734,
    0x9E3779B9, 0x7F4A7C15, 0xF39CC060, 0x5CEDC834,
    0xB7E15162, 0x8AED2A6A, 0xBF715880, 0x9CF4F3C7,
]

######################################################################
# Dynamic diffusion masks and injection functions
######################################################################

def generate_dynamic_diffusion_mask0(X: int) -> int:
    """Generate mask0 as defined in neoalzette_injection_constexpr.hpp."""
    v0 = X & 0xFFFFFFFF
    v1 = v0 ^ rotl32(v0, 2)
    v2 = v0 ^ rotl32(v1, 17)
    v3 = v0 ^ rotl32(v2, 4)
    v4 = v3 ^ rotl32(v3, 24)
    return (v2 ^ rotl32(v4, 7)) & 0xFFFFFFFF


def generate_dynamic_diffusion_mask1(X: int) -> int:
    """Generate mask1 as defined in neoalzette_injection_constexpr.hpp."""
    v0 = X & 0xFFFFFFFF
    v1 = v0 ^ rotr32(v0, 2)
    v2 = v0 ^ rotr32(v1, 17)
    v3 = v0 ^ rotr32(v2, 4)
    v4 = v3 ^ rotr32(v3, 24)
    return (v2 ^ rotr32(v4, 7)) & 0xFFFFFFFF


def cd_injection_from_B(B: int, rc0: int, rc1: int) -> tuple[int, int]:
    """
    Compute the cross‑branch injection from branch B (value domain) as in the
    C++ implementation.  The s_box term uses a NOT‑AND and constant XOR.
    """
    # RC indices as in neoalzette_core.hpp (0‑based).  We assume RC is
    # provided externally when this function is called.
    mask0 = generate_dynamic_diffusion_mask0(B)
    s_box_in_B = (B ^ RC[2]) ^ (~(B & mask0) & 0xFFFFFFFF)
    c = B & 0xFFFFFFFF
    d = (mask0 ^ rc0) & 0xFFFFFFFF
    t = c ^ d
    c ^= d ^ s_box_in_B
    d ^= rotr32(t, 16) ^ rc1
    return c & 0xFFFFFFFF, d & 0xFFFFFFFF


def cd_injection_from_A(A: int, rc0: int, rc1: int) -> tuple[int, int]:
    """
    Compute the cross‑branch injection from branch A (value domain) as in the
    C++ implementation.  The s_box term uses a NOT‑OR and constant XOR.
    """
    mask1 = generate_dynamic_diffusion_mask1(A)
    s_box_in_A = (A ^ RC[7]) ^ (~(A | mask1) & 0xFFFFFFFF)
    c = A & 0xFFFFFFFF
    d = (mask1 ^ rc0) & 0xFFFFFFFF
    t = c ^ d
    c ^= d ^ s_box_in_A
    d ^= rotl32(t, 16) ^ rc1
    return c & 0xFFFFFFFF, d & 0xFFFFFFFF

def injected_xor_term_from_branch_b(B: int) -> int:
    """Compute the XOR term injected into A from branch B."""
    c, d = cd_injection_from_B(B, (RC[2] | RC[3]), RC[3])
    return rotl32(c, 24) ^ rotl32(d, 16)


def injected_xor_term_from_branch_a(A: int) -> int:
    """Compute the XOR term injected into B from branch A."""
    c, d = cd_injection_from_A(A, (RC[7] & RC[8]), RC[8])
    return rotl32(c, 24) ^ rotl32(d, 16)

# -------------------------------
# Correct XOR-differential model:
#   g_delta(x) = f(x) ^ f(x ^ delta)
# For quadratic f, g_delta is affine:
#   g_delta(x) = L_delta(x) ^ g_delta(0)
# Reachable output deltas form an affine space:
#   g_delta(0) ^ im(L_delta)
# and each output occurs with prob 2^{-rank(L_delta)} under uniform x.
# -------------------------------

# --- fast injection rank precompute (do once) ---
_MASK32 = 0xFFFFFFFF
_INJ_F0_B = injected_xor_term_from_branch_b(0) & _MASK32
_INJ_F0_A = injected_xor_term_from_branch_a(0) & _MASK32

# lin_cols_B[i][j] = L_{delta=1<<i}(e_j)  (32 columns per i)
lin_cols_B = []
lin_cols_A = []

def _build_lin_cols(f, f0):
    cols_all = []
    for i in range(32):
        di = 1 << i
        g0 = (f0 ^ (f(di) & _MASK32)) & _MASK32
        cols = []
        for j in range(32):
            ej = 1 << j
            gij = ((f(ej) ^ f(ej ^ di)) & _MASK32)
            cols.append((gij ^ g0) & _MASK32)
        cols_all.append(tuple(cols))
    return cols_all

lin_cols_B = _build_lin_cols(injected_xor_term_from_branch_b, _INJ_F0_B)
lin_cols_A = _build_lin_cols(injected_xor_term_from_branch_a, _INJ_F0_A)

def _rank_from_cols(cols):
    basis = []
    for v in cols:
        x = v
        for b in basis:
            x = min(x, x ^ b)
        if x:
            basis.append(x)
    return len(basis)

def injection_f_B(delta: int) -> int:
    delta &= _MASK32
    return (_INJ_F0_B ^ (injected_xor_term_from_branch_b(delta) & _MASK32)) & _MASK32

def injection_f_A(delta: int) -> int:
    delta &= _MASK32
    return (_INJ_F0_A ^ (injected_xor_term_from_branch_a(delta) & _MASK32)) & _MASK32

def injection_weight_branch_b(delta: int) -> int:
    delta &= _MASK32
    if delta == 0:
        return 0
    cols = [0] * 32
    d = delta
    while d:
        lsb = d & -d
        i = lsb.bit_length() - 1
        ci = lin_cols_B[i]
        for j in range(32):
            cols[j] ^= ci[j]
        d ^= lsb
    return _rank_from_cols(cols)

def injection_weight_branch_a(delta: int) -> int:
    delta &= _MASK32
    if delta == 0:
        return 0
    cols = [0] * 32
    d = delta
    while d:
        lsb = d & -d
        i = lsb.bit_length() - 1
        ci = lin_cols_A[i]
        for j in range(32):
            cols[j] ^= ci[j]
        d ^= lsb
    return _rank_from_cols(cols)

######################################################################
# LM‑2001 variable addition model
######################################################################

def psi(alpha: int, beta: int, gamma: int) -> int:
    """Compute the ψ bitmask used in the LM‑2001 model."""
    not_alpha = (~alpha) & 0xFFFFFFFF
    return (not_alpha ^ beta) & (not_alpha ^ gamma)


def xdp_add_lm2001(alpha: int, beta: int, gamma: int) -> int:
    """
    Compute the negative log2 probability (weight) for the XOR differential
    of modular addition x+y → z when the input differences are alpha and
    beta and the output difference is gamma.  Returns -1 if the
    differential is impossible.
    """
    beta_shifted = (beta << 1) & 0xFFFFFFFF
    # shifted psi and xor condition for impossibility check
    psi_shifted = psi((alpha << 1) & 0xFFFFFFFF, beta_shifted, (gamma << 1) & 0xFFFFFFFF)
    xor_val = alpha ^ beta ^ gamma
    xor_condition = xor_val ^ beta_shifted
    if (psi_shifted & xor_condition) != 0:
        return -1  # impossible differential
    eq_val = psi(alpha, beta, gamma)
    masked_bad = (~eq_val) & 0x7FFFFFFF
    # popcount
    weight = masked_bad.bit_count()
    return weight


def find_optimal_gamma(alpha: int, beta: int) -> int:
    """
    Find the output difference γ that maximises the probability (minimises
    the weight) for the addition x+y when the input differences are
    alpha and beta.  Implements LM‑2001 Algorithm 4.
    """
    mask = 0xFFFFFFFF
    # Step 1
    r = alpha & 1
    # Step 2
    e = (~(alpha ^ beta) & (~r)) & mask
    # Step 3
    a = e & ((e << 1) & mask) & (alpha ^ ((alpha << 1) & mask))
    # Step 4: compute parity prefix p via all‑one parity on a
    # aop and aopr functions can be mimicked by reversing bits and using
    # parity trick.  Here we implement an equivalent using bitwise operations.
    # Compute aopr(a): for each bit set in a, toggle bits above it.
    def aopr(x: int) -> int:
        # reverse bits and compute all‑one parity prefix
        # Equivalent to the reference implementation in C++
        # Compute parity of prefix bits: if a bit in a is set, toggle all more
        # significant bits in p.  We propagate toggles to the left.
        p = 0
        carry = 0
        for i in range(31, -1, -1):
            bit = (x >> i) & 1
            carry ^= bit
            if carry:
                p |= (1 << i)
        return p
    p = aopr(a & mask)
    # Step 5
    a = (((a | ((a >> 1) & mask)) & (~r)) & mask)
    # Step 6
    b = (((a | e) << 1) & mask)
    # Step 7
    gamma = (((alpha ^ p) & a) | ((alpha ^ beta ^ ((alpha << 1) & mask)) & (~a) & b) | (alpha & (~a) & (~b))) & mask
    # Step 8
    gamma = ((gamma & ~1) | ((alpha ^ beta) & 1)) & mask
    return gamma


def find_optimal_gamma_with_weight(alpha: int, beta: int) -> tuple[int, int]:
    """Return (optimal_gamma, weight) for addition input differences alpha and beta."""
    gamma = find_optimal_gamma(alpha, beta)
    weight = xdp_add_lm2001(alpha, beta, gamma)
    return gamma, weight


######################################################################
# Constant subtraction model
######################################################################

def step_matrices(u_bit: int, a_bit: int):
    M0 = [[0, 0, 0, 0] for _ in range(4)]
    M1 = [[0, 0, 0, 0] for _ in range(4)]
    for s in range(4):
        c = s & 1
        cp = (s >> 1) & 1
        for x0 in (0, 1):
            x1 = x0 ^ u_bit

            y0 = x0 ^ a_bit ^ c
            y1 = x1 ^ a_bit ^ cp
            out = y0 ^ y1

            nc = (x0 & a_bit) | (x0 & c) | (a_bit & c)
            ncp = (x1 & a_bit) | (x1 & cp) | (a_bit & cp)
            ns = nc | (ncp << 1)

            if out == 0:
                M0[s][ns] += 1
            else:
                M1[s][ns] += 1
    return M0, M1

# ---- precompute step matrices once (global) ----
_STEP_ADDCONSTANT = {}
for u in (0, 1):
    for a in (0, 1):
        _STEP_ADDCONSTANT[(u, a)] = step_matrices(u, a)

@lru_cache(maxsize=200000)  # 先别太大，免得内存炸
def diff_addconst_exact_weight(alpha: int, constant: int) -> tuple[int, int, int]:
    """
    EXACT count of x in [0,2^n) such that
        (x + constant) ⊕ ((x ⊕ Δx) + constant) = Δy   (mod 2^n)
    This matches the carry-pair DP used in differential_addconst.hpp and
    the IACR ePrint 2001/052 paper.

    返回:
      (best_delta_out, wc_int, best_count)

    wc_int 是整数权重（ceil bits）:
      wc_int = ceil( 32 - log2(best_count) )
            = 32 - floor(log2(best_count))

    best_count 是精确计数（有多少个 x 满足该差分映射），用于你出报告时算浮点权重。
    """
    nbits = 32
    mask = (1 << nbits) - 1

    delta_x = alpha & mask
    constant_y = constant & mask

    def dominates(a, b) -> bool:
        return (a[0] >= b[0] and a[1] >= b[1] and a[2] >= b[2] and a[3] >= b[3])

    def prune(cands):
        items = list(cands.items())
        items.sort(key=lambda kv: (kv[0][0], kv[0][1], kv[0][2], kv[0][3], -kv[1]), reverse=True)

        kept = []
        for vec, delta_prefix in items:
            if any(dominates(kvec, vec) for kvec, _ in kept):
                continue
            new_kept = [(kvec, kdelta) for kvec, kdelta in kept if not dominates(vec, kvec)]
            new_kept.append((vec, delta_prefix))
            kept = new_kept
        return {vec: delta_prefix for vec, delta_prefix in kept}

    def apply(vec, M):
        return (
            vec[0] * M[0][0] + vec[1] * M[1][0] + vec[2] * M[2][0] + vec[3] * M[3][0],
            vec[0] * M[0][1] + vec[1] * M[1][1] + vec[2] * M[2][1] + vec[3] * M[3][1],
            vec[0] * M[0][2] + vec[1] * M[1][2] + vec[2] * M[2][2] + vec[3] * M[3][2],
            vec[0] * M[0][3] + vec[1] * M[1][3] + vec[2] * M[2][3] + vec[3] * M[3][3],
        )

    candidates: Dict[Tuple[int, int, int, int], int] = {(1, 0, 0, 0): 0}
    
    for i in range(nbits):
        u_bit = (delta_x >> i) & 1
        a_bit = (constant_y >> i) & 1
        M0, M1 = _STEP_ADDCONSTANT[(u_bit, a_bit)]

        nxt: Dict[Tuple[int, int, int, int], int] = {}
        for vec, delta_prefix in candidates.items():
            v0 = apply(vec, M0)
            if (v0[0] | v0[1] | v0[2] | v0[3]) != 0:
                d0 = delta_prefix
                prev = nxt.get(v0)
                if prev is None or d0 < prev:
                    nxt[v0] = d0

            v1 = apply(vec, M1)
            if (v1[0] | v1[1] | v1[2] | v1[3]) != 0:
                d1 = delta_prefix | (1 << i)
                prev = nxt.get(v1)
                if prev is None or d1 < prev:
                    nxt[v1] = d1

        if not nxt:
            return (alpha, 1_000_000_000, 0)  # 不可能

        candidates = prune(nxt)

    best_delta = 0
    best_count = -1

    for vec, delta_out in candidates.items():
        total = vec[0] + vec[1] + vec[2] + vec[3]
        if total > best_count or (total == best_count and delta_out < best_delta):
            best_count = total
            best_delta = delta_out

    if best_count <= 0:
        return (alpha, 1_000_000_000, 0)

    # ✅ 整数权重：ceil bits（不会引入 float，不会污染 weight_const）
    floor_log2 = best_count.bit_length() - 1
    wc_int = nbits - floor_log2

    return best_delta & mask, wc_int, best_count

def diff_subconst_best(delta_x: int, sub_constant: int) -> tuple[int, float]:
    """
    For x := x - C, pick a *likely* output difference Δy by probing random x,
    then compute its exact weight using the DP above.

    Why probe? Because finding the globally optimal Δy over 2^n candidates is
    infeasible for n=32, while your BnB search would naturally discover good
    Δy. This probe mimics that “pick high-probability Δy” behaviour.
    """

    mask = (1 << 32) - 1
    delta_x &= mask
    C = sub_constant & mask
    add_constant = ((~C) + 1) & mask  # x - C == x + (-C)
    
    best_dy, wc_int, _count= diff_addconst_exact_weight(delta_x, add_constant)
    return best_dy, wc_int

######################################################################
# NeoAlzette (one box = two subrounds) differential simulation
######################################################################

def neoalzette_round_delta(delta_a: int, delta_b: int) -> tuple[int, int, float, float, float]:
    """
    Simulate one round (two subrounds) of NeoAlzette in the difference domain.

    Returns:
      (out_delta_a, out_delta_b, weight_add, weight_const, weight_injection)

    - weight_add:     sum of variable-variable addition weights
    - weight_const:   sum of constant subtraction weights (DP on low 8 bits in this prototype)
    - weight_injection: sum of ranks of the injection mappings

    NOTE: This function was previously returning only the weights; we now also
    return the propagated (A,B) deltas so callers can chain multiple rounds.
    """
    # Copy inputs
    A, B = delta_a & 0xFFFFFFFF, delta_b & 0xFFFFFFFF
    weight_add: float = 0.0
    weight_const: float = 0.0
    weight_inj: float = 0.0

    # Subround 0
    # Step 1: B += (rotl(A,31) ^ rotl(A,17) ^ RC[0])
    add_operand = rotl32(A, 31) ^ rotl32(A, 17)
    gamma, w = find_optimal_gamma_with_weight(B, add_operand)
    B = gamma
    weight_add += float(w)

    # Step 2: A -= RC[1] (constant subtraction)
    new_delta, wc = diff_subconst_best(A, RC[1]) # Keep this !!!
    A = new_delta
    weight_const += float(wc)

    # Step 3: A ^= rotl(B,23)
    A ^= rotl32(B, 23)

    # Step 4: B ^= rotl(A,16)
    B ^= rotl32(A, 16)

    # Step 5: injection from B into A
    inj = injection_f_B(B)
    rank = injection_weight_branch_b(B)
    A ^= inj
    weight_inj += float(rank)

    # Subround 1 (swap roles)
    # Step 1: A += (rotl(B,31) ^ rotl(B,17) ^ RC[5])
    add_operand = rotl32(B, 31) ^ rotl32(B, 17)
    gamma, w = find_optimal_gamma_with_weight(A, add_operand)
    A = gamma
    weight_add += float(w)

    # Step 2: B -= RC[6]
    new_delta, wc = diff_subconst_best(B, RC[6]) # Keep this !!!
    B = new_delta
    weight_const += float(wc)

    # Step 3: B ^= rotl(A,23)
    B ^= rotl32(A, 23)

    # Step 4: A ^= rotl(B,16)
    A ^= rotl32(B, 16)

    # Step 5: injection from A into B
    inj = injection_f_A(A)
    rank = injection_weight_branch_a(A)
    B ^= inj
    weight_inj += float(rank)

    # Final constant XORs have no weight and no effect on deltas

    return A & 0xFFFFFFFF, B & 0xFFFFFFFF, weight_add, weight_const, weight_inj

######################################################################
# Alzette round differential simulation
######################################################################

def alzette_round_delta(delta_x: int, delta_y: int) -> tuple[int, int, float]:
    """
    Simulate one Alzette instance (Algorithm 1) in the difference domain.
    Returns:
      (out_delta_x, out_delta_y, weight_add)

    The constants c used in Algorithm 1 are applied via XOR and therefore
    do not contribute to the weight.
    """
    x, y = delta_x & 0xFFFFFFFF, delta_y & 0xFFFFFFFF
    weight_add: float = 0.0

    # 1: x ← x + (y ≫ 31)
    alpha = x
    beta = rotr32(y, 31)
    gamma, w = find_optimal_gamma_with_weight(alpha, beta)
    x = gamma
    weight_add += float(w)

    # 2: y ← y ⊕ (x ≫ 24)
    y ^= rotr32(x, 24)

    # 3: x ← x ⊕ c (no effect on delta)

    # 4: x ← x + (y ≫ 17)
    alpha = x
    beta = rotr32(y, 17)
    gamma, w = find_optimal_gamma_with_weight(alpha, beta)
    x = gamma
    weight_add += float(w)

    # 5: y ← y ⊕ (x ≫ 17)
    y ^= rotr32(x, 17)

    # 6: x ← x ⊕ c

    # 7: x ← x + (y ≫ 0)
    alpha = x
    beta = y
    gamma, w = find_optimal_gamma_with_weight(alpha, beta)
    x = gamma
    weight_add += float(w)

    # 8: y ← y ⊕ (x ≫ 31)
    y ^= rotr32(x, 31)

    # 9: x ← x ⊕ c

    # 10: x ← x + (y ≫ 24)
    alpha = x
    beta = rotr32(y, 24)
    gamma, w = find_optimal_gamma_with_weight(alpha, beta)
    x = gamma
    weight_add += float(w)

    # 11: y ← y ⊕ (x ≫ 16)
    y ^= rotr32(x, 16)

    # 12: x ← x ⊕ c

    return x & 0xFFFFFFFF, y & 0xFFFFFFFF, weight_add


######################################################################
# Experiment driver
######################################################################

def run_experiment(num_samples: int = 1000, seed: int = 0,
                   output_csv: str = "results.csv",
                   output_plot: str = "weight_difference.png") -> None:
    """
    Run the comparison experiment.  Samples `num_samples` random difference pairs,
    computes the weights for both ARX boxes, and writes the results to `output_csv`.
    Also produces a histogram plot of the weight differences and saves it as
    `output_plot`.
    """
    random.seed(seed)
    rows = []
    diffs = []

    for _ in range(num_samples):
        delta_a = random.getrandbits(32)
        delta_b = random.getrandbits(32)

        # Alzette now returns (outx,outy,w); we only need w here.
        _outx, _outy, w_alz = alzette_round_delta(delta_a, delta_b)

        # NeoAlzette now returns (outA,outB,wa,wc,wi); we only need weights here.
        _outA, _outB, w_add, w_const, w_inj = neoalzette_round_delta(delta_a, delta_b)
        w_neo = w_add + w_const + w_inj

        diff = w_neo - w_alz
        rows.append((delta_a, delta_b, w_alz, w_neo, diff, w_add, w_const, w_inj))
        diffs.append(diff)

    # Write CSV
    with open(output_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["delta_a", "delta_b", "weight_alzette", "weight_neoalzette",
                         "weight_difference", "weight_additions", "weight_constant", "weight_injection"])
        for row in rows:
            writer.writerow([hex(row[0]), hex(row[1]), row[2], row[3], row[4], row[5], row[6], row[7]])

    # Compute basic statistics
    mean_diff = sum(diffs) / len(diffs)
    min_diff = min(diffs)
    max_diff = max(diffs)
    print(f"Experiment completed for {num_samples} samples.")
    print(f"Average weight difference (NeoAlzette - Alzette): {mean_diff:.3f} bits")
    print(f"Min difference: {min_diff:.3f} bits, Max difference: {max_diff:.3f} bits")

    # Try to produce a histogram plot with matplotlib if available
    try:
        import matplotlib.pyplot as plt
        import numpy as np  # noqa: F401
        plt.figure(figsize=(8, 4))
        plt.hist(diffs, bins=40, edgecolor='black')
        plt.title(f'Distribution of Weight Differences (NeoAlzette - Alzette)\nN={num_samples}, seed={seed}')
        plt.xlabel('Weight difference (bits)')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_plot)
        plt.close()
        print(f"Plot saved to {output_plot}")
    except ImportError:
        print("matplotlib not available; skipping plot generation.")

def run_round_experiment(
    max_rounds: int = 8,
    num_samples: int = 20000,
    seed: int = 0,
    output_csv: str = "results_by_round.csv",
    *,
    reuse_samples: bool = True,
    use_transition_cache: bool = False,
) -> None:
    """
    Per-round trend experiment.

    Two modes:

    1) reuse_samples=True  (FAST, recommended)
       - Sample `num_samples` random (Δa, Δb) ONCE.
       - For each pair, propagate up to `max_rounds` boxes, accumulating
         cumulative weights after each box.
       - Report mean cumulative weight at r=1..max_rounds.

       Complexity: O(num_samples * max_rounds)

    2) reuse_samples=False (matches the older "fresh random samples per r" idea)
       - For each r, draw new random pairs and simulate exactly r boxes.
       - Much slower: O(num_samples * max_rounds*(max_rounds+1)/2)

    Optional: use_transition_cache
      Cache (state -> next_state, weights) transitions. With 32-bit random states,
      collisions are rare, so speedup may be modest; the BIG speedup comes from
      reuse_samples=True.
    """
    random.seed(seed)

    # local bindings (tiny speed win)
    alz = alzette_round_delta
    neo = neoalzette_round_delta

    rows: list[tuple[int, float, float, float]] = []

    if reuse_samples:
        # Pre-sample once
        samples = [(random.getrandbits(32), random.getrandbits(32)) for _ in range(num_samples)]

        sum_alz = [0.0] * max_rounds
        sum_neo = [0.0] * max_rounds

        cache_alz: dict[tuple[int, int], tuple[int, int, float]] = {}
        cache_neo: dict[tuple[int, int], tuple[int, int, float, float, float]] = {}

        for da0, db0 in samples:
            da_alz, db_alz = da0, db0
            da_neo, db_neo = da0, db0

            cum_alz = 0.0
            cum_neo = 0.0

            for r in range(max_rounds):
                if use_transition_cache:
                    key = (da_alz, db_alz)
                    v = cache_alz.get(key)
                    if v is None:
                        v = alz(da_alz, db_alz)
                        cache_alz[key] = v
                    da_alz, db_alz, w = v
                else:
                    da_alz, db_alz, w = alz(da_alz, db_alz)
                cum_alz += w

                if use_transition_cache:
                    key = (da_neo, db_neo)
                    v2 = cache_neo.get(key)
                    if v2 is None:
                        v2 = neo(da_neo, db_neo)
                        cache_neo[key] = v2
                    da_neo, db_neo, wa, wc, wi = v2
                else:
                    da_neo, db_neo, wa, wc, wi = neo(da_neo, db_neo)
                cum_neo += (wa + wc + wi)

                sum_alz[r] += cum_alz
                sum_neo[r] += cum_neo

        for r in range(max_rounds):
            mean_alz = sum_alz[r] / num_samples
            mean_neo = sum_neo[r] / num_samples
            mean_diff = mean_neo - mean_alz
            rows.append((r + 1, mean_alz, mean_neo, mean_diff))
            print(f"[rounds={r+1}] mean_alz={mean_alz:.3f}, mean_neo={mean_neo:.3f}, mean_diff={mean_diff:.3f}")

    else:
        # Slow / legacy mode: fresh random pairs per r
        cache_alz: dict[tuple[int, int], tuple[int, int, float]] = {}
        cache_neo: dict[tuple[int, int], tuple[int, int, float, float, float]] = {}

        for r in range(1, max_rounds + 1):
            w_alz_list: list[float] = []
            w_neo_list: list[float] = []

            for _ in range(num_samples):
                da0 = random.getrandbits(32)
                db0 = random.getrandbits(32)

                da_alz, db_alz = da0, db0
                da_neo, db_neo = da0, db0

                w_alz = 0.0
                w_neo = 0.0

                for _k in range(r):
                    if use_transition_cache:
                        key = (da_alz, db_alz)
                        v = cache_alz.get(key)
                        if v is None:
                            v = alz(da_alz, db_alz)
                            cache_alz[key] = v
                        da_alz, db_alz, w = v
                    else:
                        da_alz, db_alz, w = alz(da_alz, db_alz)
                    w_alz += w

                    if use_transition_cache:
                        key = (da_neo, db_neo)
                        v2 = cache_neo.get(key)
                        if v2 is None:
                            v2 = neo(da_neo, db_neo)
                            cache_neo[key] = v2
                        da_neo, db_neo, wa, wc, wi = v2
                    else:
                        da_neo, db_neo, wa, wc, wi = neo(da_neo, db_neo)
                    w_neo += (wa + wc + wi)

                w_alz_list.append(w_alz)
                w_neo_list.append(w_neo)

            mean_alz = sum(w_alz_list) / len(w_alz_list)
            mean_neo = sum(w_neo_list) / len(w_neo_list)
            mean_diff = mean_neo - mean_alz
            rows.append((r, mean_alz, mean_neo, mean_diff))
            print(f"[rounds={r}] mean_alz={mean_alz:.3f}, mean_neo={mean_neo:.3f}, mean_diff={mean_diff:.3f}")

    # Write CSV
    with open(output_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["rounds", "mean_weight_alzette", "mean_weight_neoalzette", "mean_weight_difference"])
        for r, ma, mn, md in rows:
            writer.writerow([r, ma, mn, md])

    print(f"Round experiment saved to {output_csv}")

######################################################################
# Plotting / statistics helper for multi-round trend experiments
######################################################################


def plot_round_trend(input_csv: str = "results_by_round.csv",
                     output_plot: str = "weight_diff_trend.png",
                     num_samples: int | None = None,
                     seed: int | None = None) -> None:
    """
    Plot two lines (Alzette vs NeoAlzette) from a per-round CSV.

    Accept either of these column sets (auto-detected):
      A) rounds, mean_weight_alzette, mean_weight_neoalzette, mean_diff, ...
      B) rounds, mean_weight_alzette, mean_weight_neoalzette, mean_weight_difference, ...

    The goal is presentation only:
    - two lines (Alzette vs NeoAlzette)
    - annotate sampling scale (N, seed) when provided
    """
    
    import csv as _csv

    rows: list[dict[str, str]] = []
    with open(input_csv, "r", newline="") as f:
        reader = _csv.DictReader(f)
        for r in reader:
            rows.append(r)
    if not rows:
        raise ValueError(f"No rows found in {input_csv}")

    rounds = [int(r["rounds"]) for r in rows]
    mean_alz = [float(r["mean_weight_alzette"]) for r in rows]
    mean_neo = [float(r["mean_weight_neoalzette"]) for r in rows]

    # diff column compatibility
    if "mean_weight_difference" in rows[0]:
        mean_diff = [float(r["mean_weight_difference"]) for r in rows]
    elif "mean_diff" in rows[0]:
        mean_diff = [float(r["mean_diff"]) for r in rows]
    else:
        mean_diff = [n - a for n, a in zip(mean_neo, mean_alz)]

    print(f"[trend] Loaded {len(rows)} round-points from {input_csv}")
    if num_samples is not None:
        print(f"[trend] N per round = {num_samples}")
    if seed is not None:
        print(f"[trend] seed = {seed}")
    print(f"[trend] last-round mean(Neo-Alz) = {mean_diff[-1]:.3f} bits")

    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(8, 4))
        plt.plot(rounds, mean_alz, marker="o", label="Alzette (mean weight)")
        plt.plot(rounds, mean_neo, marker="o", label="NeoAlzette (mean weight)")
        title = "Mean Differential Weight vs Rounds"
        meta = []
        if num_samples is not None:
            meta.append(f"N={num_samples} per round")
        if seed is not None:
            meta.append(f"seed={seed}")
        if meta:
            title += "\n" + ", ".join(meta)
        plt.title(title)
        plt.xlabel("Number of boxes (rounds)")
        plt.ylabel("Mean weight (bits)")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(output_plot)
        plt.close()
        print(f"[trend] Plot saved to {output_plot}")
    except ImportError:
        print("[trend] matplotlib not available; skipping plot generation.")
        


def delta_injected_xor_term_from_branch_b(base_B: int, delta: int) -> int:
    return injected_xor_term_from_branch_b(base_B ^ delta) ^ injected_xor_term_from_branch_b(base_B)

def delta_injected_xor_term_from_branch_a(base_A: int, delta: int) -> int:
    return injected_xor_term_from_branch_a(base_A ^ delta) ^ injected_xor_term_from_branch_a(base_A)

def check_true_injection_delta_linearity(trials: int = 2000, seed: int = 0) -> None:
    import random
    random.seed(seed)

    flag = True
    flag2 = True
    # Check: for fixed base, is delta-map linear in delta?
    for _ in range(trials):
        base = random.getrandbits(32)
        u = random.getrandbits(32)
        v = random.getrandbits(32)
        du = delta_injected_xor_term_from_branch_b(base, u)
        dv = delta_injected_xor_term_from_branch_b(base, v)
        duv = delta_injected_xor_term_from_branch_b(base, u ^ v)
        if duv != (du ^ dv):
            print("Branch B delta-map is NOT linear (expected for dynamic AND/OR).")
            flag = False
            break
    if flag :
        print("Branch B delta-map looked linear in this sample (surprising).")

    random.seed(seed)
    for _ in range(trials):
        base = random.getrandbits(32)
        u = random.getrandbits(32)
        v = random.getrandbits(32)
        du = delta_injected_xor_term_from_branch_a(base, u)
        dv = delta_injected_xor_term_from_branch_a(base, v)
        duv = delta_injected_xor_term_from_branch_a(base, u ^ v)
        if duv != (du ^ dv):
            print("Branch A delta-map is NOT linear (expected for dynamic AND/OR).")
            flag2 = False
            break
    if flag2 :
        print("Branch A delta-map looked linear in this sample (surprising).")
        
def check_derivative_is_affine(f, trials: int = 200, seed: int = 0) -> None:
    rng = random.Random(seed)
    for _ in range(trials):
        delta = rng.getrandbits(32)
        x = rng.getrandbits(32)
        y = rng.getrandbits(32)

        def g(z: int) -> int:
            return (f(z) ^ f(z ^ delta)) & 0xFFFFFFFF

        # g affine  <=>  g(x^y) == g(x) ^ g(y) ^ g(0)
        if g(x ^ y) != ((g(x) ^ g(y) ^ g(0)) & 0xFFFFFFFF):
            print("Derivative is NOT affine -> bug (unexpected for quadratic injection).")
            return
    print("Derivative behaves affine (expected).")

def check_f_is_nonlinear(f, trials: int = 200, seed: int = 0) -> None:
    rng = random.Random(seed)
    for _ in range(trials):
        u = rng.getrandbits(32)
        v = rng.getrandbits(32)
        if (f(u ^ v) & 0xFFFFFFFF) != ((f(u) ^ f(v)) & 0xFFFFFFFF):
            print("f is NOT linear (expected).")
            return
    print("f seems linear under this test set (unexpected).")


if __name__ == "__main__":
    """
    When executed as a script, perform both the single‑round sampling experiment
    and the per‑round trend experiment.  The single‑round experiment is run
    first to produce a CSV and histogram comparing the differential weight
    distributions of the Alzette and NeoAlzette ARX boxes.  The per‑round
    trend experiment then computes mean weights across multiple boxes.  These
    functions are exposed separately so that other scripts may import and
    call them directly without executing heavy workloads on import.

    Note: the chosen parameters mirror the original defaults (1000 samples
    for the single‑round experiment and 200000 samples per round for the
    trend experiment) and should not be reduced here unless explicitly
    requested by the caller.  If you wish to run smaller experiments for
    testing purposes, write a separate wrapper script that calls
    `run_experiment()` and `run_round_experiment()` with custom parameters.
    """

    #check_true_injection_delta_linearity()
    #check_f_is_nonlinear(injected_xor_term_from_branch_a, 200, 1)
    #check_derivative_is_affine(injected_xor_term_from_branch_a, 200, 1)
    #check_f_is_nonlinear(injected_xor_term_from_branch_b, 200, 1)
    #check_derivative_is_affine(injected_xor_term_from_branch_b, 200, 1)

    # Run the multi‑box (per‑round) experiment.  This remains large by
    # default (200000 samples per round) to match the original intent.  Use
    # a wrapper script to supply smaller values if needed for testing.
    run_round_experiment(
        max_rounds=8,
        num_samples=80000,
        seed=0,
        output_csv="results_by_round.csv",
        reuse_samples=True,
        use_transition_cache=False
    )

    # Generate a trend plot from the per‑round CSV.  Passing the sample
    # count and seed helps annotate the plot title.  The
    # `plot_round_trend` function gracefully degrades when matplotlib is
    # unavailable.
    plot_round_trend(
        input_csv="results_by_round.csv",
        output_plot="weight_diff_trend_2lines.png",
        num_samples=80000,
        seed=0,
    )

    # Run a single‑box experiment with the default sample count.  This
    # generates a CSV and histogram plot summarising the weight
    # distribution.  Adjust `num_samples` only in wrapper scripts to
    # perform smaller or larger experiments without modifying this file.
    run_experiment(
        num_samples=80000,
        seed=0,
        output_csv="results.csv",
        output_plot="weight_difference.png",
    )
